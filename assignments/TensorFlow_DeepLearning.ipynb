{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "MMAI5000_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzH2hvYRwS8n"
      },
      "source": [
        "# AI Fundamentals - Assignment 3\n",
        "\n",
        "This assignment requires you to use [Tensorflow](https://www.tensorflow.org) and [Keras](https://keras.io/). Keras is a high-level Deep Learning API written in Python working as an interface to TensorFlow.\n",
        "\n",
        "This assignment is divided in two parts. In the first part you will learn about Keras with the help of the example below and the Keras [documentation](https://keras.io/). In the second part, you will practise training a Deep Learning model.\n",
        "\n",
        "## How to submit\n",
        "Submit by uploading this notebook to Canvas. It should include **plots**, **results** and **code** showing how the results were genereated.  Remember to name your file(s) appropriately.\n",
        "It is due on 11:59 of December 9, 2020.\n",
        "\n",
        "## Installation\n",
        "Instructions can be found here:\n",
        "* [Tensorflow](https://www.tensorflow.org/install/)\n",
        "\n",
        "Since Tensorflow 2.0, Keras is included in Tensorflow and will be automatically installed with Tensorflow. It can be accessed as ```tensorflow.keras```\n",
        "\n",
        "I recommend using ```pip```. For Tensorflow is it sufficient to install the CPU version. The GPU version requires a good workstation with high-end Nvidia GPU(s), and it is not necessary for this tutorial.\n",
        "\n",
        "If you're using a virtualenv:\n",
        "```\n",
        "pip3 install tensorflow\n",
        "```\n",
        "Add ```sudo``` for a systemwide installation (i.e. no ```virtualenv```).\n",
        "```\n",
        "sudo pip3 install tensorflow\n",
        "```\n",
        "Make sure that you have ```sklearn```, ```matplotlib``` and ```numpy``` installed, too.\n",
        "\n",
        "\n",
        "## Part 1 - understand a model\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
        "\n",
        "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
        "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
        "\n",
        "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
        "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
        "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
        " -->\n",
        "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
        "\n",
        "#### Mini-overview over popular optimizers\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
        "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
        "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
        "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
        "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
        "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
        "\n",
        "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
        "\n",
        "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
        "\n",
        "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
        "\n",
        "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
        "<b>Figure 2.</b> Comparison of six different optimizers.\n",
        "\n",
        "\n",
        "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
        "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
        "\n",
        "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
        "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epl9xrNnwS8o"
      },
      "source": [
        "###!pip install -U --ignore-installed wrapt enum34 simplejson netaddr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4CDz0WwS8o"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "# for the random seed\n",
        "import tensorflow as tf\n",
        "\n",
        "# set the random seeds to get reproducible results\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X, y = X[:1000], y[:1000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9mrSqM7wS8o",
        "outputId": "767878fc-fafc-4dd8-fc5d-abda076dfd14"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcmfIliXwS8q",
        "outputId": "732dfe3c-0e40-4a67-edbb-7967011c974a"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfREkzqBwS8q",
        "outputId": "3a47395f-cad0-4acf-caae-c81bc57836fc"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['5', '0', '4', '1', '9', '2', '1', '3', '1', '4', '3', '5', '3',\n",
              "       '6', '1', '7', '2', '8', '6', '9', '4', '0', '9', '1', '1', '2',\n",
              "       '4', '3', '2', '7', '3', '8', '6', '9', '0', '5', '6', '0', '7',\n",
              "       '6', '1', '8', '7', '9', '3', '9', '8', '5', '9', '3', '3', '0',\n",
              "       '7', '4', '9', '8', '0', '9', '4', '1', '4', '4', '6', '0', '4',\n",
              "       '5', '6', '1', '0', '0', '1', '7', '1', '6', '3', '0', '2', '1',\n",
              "       '1', '7', '9', '0', '2', '6', '7', '8', '3', '9', '0', '4', '6',\n",
              "       '7', '4', '6', '8', '0', '7', '8', '3', '1', '5', '7', '1', '7',\n",
              "       '1', '1', '6', '3', '0', '2', '9', '3', '1', '1', '0', '4', '9',\n",
              "       '2', '0', '0', '2', '0', '2', '7', '1', '8', '6', '4', '1', '6',\n",
              "       '3', '4', '5', '9', '1', '3', '3', '8', '5', '4', '7', '7', '4',\n",
              "       '2', '8', '5', '8', '6', '7', '3', '4', '6', '1', '9', '9', '6',\n",
              "       '0', '3', '7', '2', '8', '2', '9', '4', '4', '6', '4', '9', '7',\n",
              "       '0', '9', '2', '9', '5', '1', '5', '9', '1', '2', '3', '2', '3',\n",
              "       '5', '9', '1', '7', '6', '2', '8', '2', '2', '5', '0', '7', '4',\n",
              "       '9', '7', '8', '3', '2', '1', '1', '8', '3', '6', '1', '0', '3',\n",
              "       '1', '0', '0', '1', '7', '2', '7', '3', '0', '4', '6', '5', '2',\n",
              "       '6', '4', '7', '1', '8', '9', '9', '3', '0', '7', '1', '0', '2',\n",
              "       '0', '3', '5', '4', '6', '5', '8', '6', '3', '7', '5', '8', '0',\n",
              "       '9', '1', '0', '3', '1', '2', '2', '3', '3', '6', '4', '7', '5',\n",
              "       '0', '6', '2', '7', '9', '8', '5', '9', '2', '1', '1', '4', '4',\n",
              "       '5', '6', '4', '1', '2', '5', '3', '9', '3', '9', '0', '5', '9',\n",
              "       '6', '5', '7', '4', '1', '3', '4', '0', '4', '8', '0', '4', '3',\n",
              "       '6', '8', '7', '6', '0', '9', '7', '5', '7', '2', '1', '1', '6',\n",
              "       '8', '9', '4', '1', '5', '2', '2', '9', '0', '3', '9', '6', '7',\n",
              "       '2', '0', '3', '5', '4', '3', '6', '5', '8', '9', '5', '4', '7',\n",
              "       '4', '2', '7', '3', '4', '8', '9', '1', '9', '2', '8', '7', '9',\n",
              "       '1', '8', '7', '4', '1', '3', '1', '1', '0', '2', '3', '9', '4',\n",
              "       '9', '2', '1', '6', '8', '4', '7', '7', '4', '4', '9', '2', '5',\n",
              "       '7', '2', '4', '4', '2', '1', '9', '7', '2', '8', '7', '6', '9',\n",
              "       '2', '2', '3', '8', '1', '6', '5', '1', '1', '0', '2', '6', '4',\n",
              "       '5', '8', '3', '1', '5', '1', '9', '2', '7', '4', '4', '4', '8',\n",
              "       '1', '5', '8', '9', '5', '6', '7', '9', '9', '3', '7', '0', '9',\n",
              "       '0', '6', '6', '2', '3', '9', '0', '7', '5', '4', '8', '0', '9',\n",
              "       '4', '1', '2', '8', '7', '1', '2', '6', '1', '0', '3', '0', '1',\n",
              "       '1', '8', '2', '0', '3', '9', '4', '0', '5', '0', '6', '1', '7',\n",
              "       '7', '8', '1', '9', '2', '0', '5', '1', '2', '2', '7', '3', '5',\n",
              "       '4', '9', '7', '1', '8', '3', '9', '6', '0', '3', '1', '1', '2',\n",
              "       '6', '3', '5', '7', '6', '8', '3', '9', '5', '8', '5', '7', '6',\n",
              "       '1', '1', '3', '1', '7', '5', '5', '5', '2', '5', '8', '7', '0',\n",
              "       '9', '7', '7', '5', '0', '9', '0', '0', '8', '9', '2', '4', '8',\n",
              "       '1', '6', '1', '6', '5', '1', '8', '3', '4', '0', '5', '5', '8',\n",
              "       '3', '6', '2', '3', '9', '2', '1', '1', '5', '2', '1', '3', '2',\n",
              "       '8', '7', '3', '7', '2', '4', '6', '9', '7', '2', '4', '2', '8',\n",
              "       '1', '1', '3', '8', '4', '0', '6', '5', '9', '3', '0', '9', '2',\n",
              "       '4', '7', '1', '2', '9', '4', '2', '6', '1', '8', '9', '0', '6',\n",
              "       '6', '7', '9', '9', '8', '0', '1', '4', '4', '6', '7', '1', '5',\n",
              "       '7', '0', '3', '5', '8', '4', '7', '1', '2', '5', '9', '5', '6',\n",
              "       '7', '5', '9', '8', '8', '3', '6', '9', '7', '0', '7', '5', '7',\n",
              "       '1', '1', '0', '7', '9', '2', '3', '7', '3', '2', '4', '1', '6',\n",
              "       '2', '7', '5', '5', '7', '4', '0', '2', '6', '3', '6', '4', '0',\n",
              "       '4', '2', '6', '0', '0', '0', '0', '3', '1', '6', '2', '2', '3',\n",
              "       '1', '4', '1', '5', '4', '6', '4', '7', '2', '8', '7', '9', '2',\n",
              "       '0', '5', '1', '4', '2', '8', '3', '2', '4', '1', '5', '4', '6',\n",
              "       '0', '7', '9', '8', '4', '9', '8', '0', '1', '1', '0', '2', '2',\n",
              "       '3', '2', '4', '4', '5', '8', '6', '5', '7', '7', '8', '8', '9',\n",
              "       '7', '4', '7', '3', '2', '0', '8', '6', '8', '6', '1', '6', '8',\n",
              "       '9', '4', '0', '9', '0', '4', '1', '5', '4', '7', '5', '3', '7',\n",
              "       '4', '9', '8', '5', '8', '6', '3', '8', '6', '9', '9', '1', '8',\n",
              "       '3', '5', '8', '6', '5', '9', '7', '2', '5', '0', '8', '5', '1',\n",
              "       '1', '0', '9', '1', '8', '6', '7', '0', '9', '3', '0', '8', '8',\n",
              "       '9', '6', '7', '8', '4', '7', '5', '9', '2', '6', '7', '4', '5',\n",
              "       '9', '2', '3', '1', '6', '3', '9', '2', '2', '5', '6', '8', '0',\n",
              "       '7', '7', '1', '9', '8', '7', '0', '9', '9', '4', '6', '2', '8',\n",
              "       '5', '1', '4', '1', '5', '5', '1', '7', '3', '6', '4', '3', '2',\n",
              "       '5', '6', '4', '4', '0', '4', '4', '6', '7', '2', '4', '3', '3',\n",
              "       '8', '0', '0', '3', '2', '2', '9', '8', '2', '3', '7', '0', '1',\n",
              "       '1', '0', '2', '3', '3', '8', '4', '3', '5', '7', '6', '4', '7',\n",
              "       '7', '8', '5', '9', '7', '0', '3', '1', '6', '2', '4', '3', '4',\n",
              "       '4', '7', '5', '9', '6', '9', '0', '7', '1', '4', '2', '7', '3',\n",
              "       '6', '7', '5', '8', '4', '5', '5', '2', '7', '1', '1', '5', '6',\n",
              "       '8', '5', '8', '4', '0', '7', '9', '9', '2', '9', '7', '7', '8',\n",
              "       '7', '4', '2', '6', '9', '1', '7', '0', '6', '4', '2', '5', '7',\n",
              "       '0', '7', '1', '0', '3', '7', '6', '5', '0', '6', '1', '5', '1',\n",
              "       '7', '8', '5', '0', '3', '4', '7', '7', '5', '7', '8', '6', '9',\n",
              "       '3', '8', '6', '1', '0', '9', '7', '1', '3', '0', '5', '6', '4',\n",
              "       '4', '2', '4', '4', '3', '1', '7', '7', '6', '0', '3', '6'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5nIAyn1wS8r"
      },
      "source": [
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "# Normalize\n",
        "X = X / 255\n",
        "# number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
        "\n",
        "num_tot = y.shape[0]\n",
        "num_train = y_train.shape[0]\n",
        "num_test = y_test.shape[0]\n",
        "\n",
        "y_oh = np.zeros((num_tot, num_classes))\n",
        "y_oh[range(num_tot), y] = 1\n",
        "\n",
        "y_oh_train = np.zeros((num_train, num_classes))\n",
        "y_oh_train[range(num_train), y_train] = 1\n",
        "\n",
        "y_oh_test = np.zeros((num_test, num_classes))\n",
        "y_oh_test[range(num_test), y_test] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebsgMZ3pwS8r",
        "outputId": "737ebcd0-dfe3-477d-df46-dcc425da9f05"
      },
      "source": [
        "y_oh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tWirPT_0wWZ",
        "outputId": "c9052345-c5f1-408f-cea1-71e0b1735ddf"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVKmNjFdwS8r"
      },
      "source": [
        "### Question 1\n",
        "**The data set**\n",
        "\n",
        "Plot a three examples from the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5k23wb2wS8r"
      },
      "source": [
        "* What type of data are in the data set?\n",
        "\n",
        "    <span style=\"color:red\"> The data set stores 1000 examples of a 28x28 pixels image from number 0 to number 9 </span>\n",
        "    \n",
        "\n",
        "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
        "\n",
        "    <span style=\"color:red\">  X.reshape(X.shape[0], 28, 28, 1) Reshape X from 1000rows x 784columns to 1000 rows of (28x28 matrix)</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAkcBN53wS8r"
      },
      "source": [
        "\n",
        "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
        "```\n",
        "    y_oh = np.zeros((num_tot, num_classes))\n",
        "    y_oh[range(num_tot), y] = 1\n",
        "```\n",
        "Print out a few rows of ```y``` next to ```y_oh```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPKsIOXwS8r",
        "outputId": "f8c8a0ad-4267-4d94-dd80-59ae407ae770"
      },
      "source": [
        "for i in range(20):\n",
        "    print(\"y: \", y[i], \"      y_oh: \", y_oh[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y:  5       y_oh:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "y:  0       y_oh:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  4       y_oh:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "y:  1       y_oh:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  9       y_oh:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "y:  2       y_oh:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  1       y_oh:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  3       y_oh:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "y:  1       y_oh:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  4       y_oh:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "y:  3       y_oh:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "y:  5       y_oh:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "y:  3       y_oh:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "y:  6       y_oh:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "y:  1       y_oh:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  7       y_oh:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "y:  2       y_oh:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y:  8       y_oh:  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "y:  6       y_oh:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "y:  9       y_oh:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXbG_YrRwS8r"
      },
      "source": [
        "\n",
        "* What is the relationship between ```y``` and ```y_oh```?\n",
        "\n",
        "    <span style=\"color:red\"> y_oh is the representation array of y, its length equal to the classes of y , and filled with zeros.  We can see the '1''s index of y_oh equals to the y value.</span>\n",
        "    \n",
        "    \n",
        "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
        "\n",
        "    <span style=\"color:red\"> One Hot Encoding.  One hot encoding allows us to convert categorical data to be more expressive numbers since some machine learning algorithms cannot work with categorical data directly. When we are working with categorical data ,we can encoded by one hot encoding so we can analyze categorical crossentropy loss. </span>\n",
        "    \n",
        "    \n",
        "* Plot three data examples in the same figure and set the correct label as title. \n",
        "    * It should be possible to see what the data represent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "W0CSSOJ8wS8r",
        "outputId": "c1ca933c-230c-4a4a-c297-c2bbf95587ad"
      },
      "source": [
        "for i in range(0,3):\n",
        "    img = X[i].reshape((28,28))\n",
        "    plt.subplot(131+i)\n",
        "    plt.imshow(img, cmap=\"Greys\")\n",
        "    plt.title(y[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffXqz157wS8r"
      },
      "source": [
        "### Question 2\n",
        "**The model**\n",
        "\n",
        "Below is some code for bulding and training a model with Keras.\n",
        "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
        "\n",
        "    <span style=\"color:red\"> CNN </span>\n",
        "    \n",
        "    \n",
        "* What does ```Dropout()``` do?\n",
        "\n",
        "    <span style=\"color:red\">  Dropout Layer randomly sets input units to 0 with a frequency of rate at each step during training time to reduce overfitting </span>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the hidden layers?\n",
        "\n",
        "    <span style=\"color:red\"> RELU </span>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the output layer?\n",
        "\n",
        "    <span style=\"color:red\"> Softmax </span>\n",
        "\n",
        "\n",
        "* Why are two different activation functions used?\n",
        "\n",
        "    <span style=\"color:red\"> Hidenlayers have many neurons, RELU's simplicity can increase the speed. For output layer,we can not use RELU here which will generate 0 or 1, since we need multiclass classification,we need Softmax function to generate probability distribution containing different probability values for different classes,  </span>\n",
        "\n",
        "\n",
        "* What optimizer is used in the model below?\n",
        "\n",
        "    <span style=\"color:red\"> SGD,Stochastic gradient descent Optimizer </span>\n",
        "\n",
        "\n",
        "* How often are the weights updated (i.e. after how many data examples)?\n",
        "\n",
        "    <span style=\"color:red\"> learning rate is 0.01 so weights are updated after 1% of data examples. in this case train data has 800 examples, so weight will be updated every 8 examples </span>\n",
        "\n",
        "\n",
        "* What loss function is used?\n",
        "\n",
        "    <span style=\"color:red\"> This case need a Multi-Class Classification Loss Function, here it used Categorical Cross-Entropy as loss function which can  compute how bad the model performs.</span>\n",
        "\n",
        "\n",
        "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
        "    <span style=\"color:red\"> Conv2d_1:16 * 3 * 3+16=160 Conv2d_2:16 * 32 * 3 * 3+32 =4640 Dense_1= 5 * 5 * 32 * 128 +128=102528 Dens2= 128 * 10 +10 =1290\n",
        "Other layers:0  Total:108618</span>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhMFk-jYwS8r",
        "outputId": "6c83e150-2bc8-416d-c8b6-461277633634"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
        "\n",
        "# Evaluate performance\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.1367\n",
            "Epoch 2/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.0638\n",
            "Epoch 3/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.5037\n",
            "Epoch 4/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.3274\n",
            "Epoch 5/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.2436\n",
            "Epoch 6/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.1925\n",
            "Epoch 7/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.1752\n",
            "Epoch 8/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.1191\n",
            "Epoch 9/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0964\n",
            "Epoch 10/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0674\n",
            "Epoch 11/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0534\n",
            "Epoch 12/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0434\n",
            "Epoch 13/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0304\n",
            "Epoch 14/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0210\n",
            "Epoch 15/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0152\n",
            "Epoch 16/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0193\n",
            "Epoch 17/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0113\n",
            "Epoch 18/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0076\n",
            "Epoch 19/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0066\n",
            "Epoch 20/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0044\n",
            "Epoch 21/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0039\n",
            "Epoch 22/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0031\n",
            "Epoch 23/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0030\n",
            "Epoch 24/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0026\n",
            "Epoch 25/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0023\n",
            "Epoch 26/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0022\n",
            "Epoch 27/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0020\n",
            "Epoch 28/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0019\n",
            "Epoch 29/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0017\n",
            "Epoch 30/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0016\n",
            "Epoch 31/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0015\n",
            "Epoch 32/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0015\n",
            "Epoch 33/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0014\n",
            "Epoch 34/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0013\n",
            "Epoch 35/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0012\n",
            "Epoch 36/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0012\n",
            "Epoch 37/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0011\n",
            "Epoch 38/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0011\n",
            "Epoch 39/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0010\n",
            "Epoch 40/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 9.7721e-04\n",
            "Epoch 41/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 9.7695e-04\n",
            "Epoch 42/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 9.3003e-04\n",
            "Epoch 43/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 8.7942e-04\n",
            "Epoch 44/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 8.5037e-04\n",
            "Epoch 45/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 8.1755e-04\n",
            "Epoch 46/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 7.9134e-04\n",
            "Epoch 47/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.8248e-04\n",
            "Epoch 48/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.6190e-04\n",
            "Epoch 49/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.2741e-04\n",
            "Epoch 50/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.0065e-04\n",
            "Epoch 51/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 6.8190e-04\n",
            "Epoch 52/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 6.5684e-04\n",
            "Epoch 53/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 6.3774e-04\n",
            "Epoch 54/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 6.2655e-04\n",
            "Epoch 55/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 6.1481e-04\n",
            "Epoch 56/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 5.9605e-04\n",
            "Epoch 57/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 5.7725e-04\n",
            "Epoch 58/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 5.6648e-04\n",
            "Epoch 59/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 5.4801e-04\n",
            "Epoch 60/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 5.3758e-04\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4677\n",
            "Accuracy: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey2G5gKQwS8s",
        "outputId": "44a90f4b-1ad8-4e6c-d5c4-e27875ab577b"
      },
      "source": [
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 108,618\n",
            "Trainable params: 108,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YCG1RbRwS8s"
      },
      "source": [
        "## Part 2 - train a model\n",
        "\n",
        "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
        "\n",
        "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
        "\n",
        "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
        "\n",
        "When training, an **epoch** is one pass through the full training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjEY90jOwS8s"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
        "    * Remember to first reset the weights (```model.reset_states()```), otherwise the training just continues from where it was stopped earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "annSvE6-wS8s"
      },
      "source": [
        "model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nufLEUadwS8s",
        "outputId": "2111276a-bfa9-40ca-fa43-973cbb458524"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "# Train the model\n",
        "\n",
        "history = model.fit(X_train, y_oh_train,validation_data=(X_test,y_oh_test),batch_size=32, epochs=150)\n",
        "\n",
        "# Evaluate performance\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.1279 - val_loss: 1.7812\n",
            "Epoch 2/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.0279 - val_loss: 0.7918\n",
            "Epoch 3/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.5010 - val_loss: 0.4853\n",
            "Epoch 4/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.3316 - val_loss: 0.3674\n",
            "Epoch 5/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.2474 - val_loss: 0.4285\n",
            "Epoch 6/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.2228 - val_loss: 0.3760\n",
            "Epoch 7/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.1669 - val_loss: 0.3550\n",
            "Epoch 8/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.1305 - val_loss: 0.3529\n",
            "Epoch 9/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.1018 - val_loss: 0.3083\n",
            "Epoch 10/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0764 - val_loss: 0.3670\n",
            "Epoch 11/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0560 - val_loss: 0.3779\n",
            "Epoch 12/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0547 - val_loss: 0.3347\n",
            "Epoch 13/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0351 - val_loss: 0.3355\n",
            "Epoch 14/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0291 - val_loss: 0.3773\n",
            "Epoch 15/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0318 - val_loss: 0.3683\n",
            "Epoch 16/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0252 - val_loss: 0.3750\n",
            "Epoch 17/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0164 - val_loss: 0.3083\n",
            "Epoch 18/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0087 - val_loss: 0.3438\n",
            "Epoch 19/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0069 - val_loss: 0.3501\n",
            "Epoch 20/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0046 - val_loss: 0.3726\n",
            "Epoch 21/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0034 - val_loss: 0.3642\n",
            "Epoch 22/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0029 - val_loss: 0.3717\n",
            "Epoch 23/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0027 - val_loss: 0.3664\n",
            "Epoch 24/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.3806\n",
            "Epoch 25/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0021 - val_loss: 0.3783\n",
            "Epoch 26/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0020 - val_loss: 0.3848\n",
            "Epoch 27/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0018 - val_loss: 0.3901\n",
            "Epoch 28/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0017 - val_loss: 0.3885\n",
            "Epoch 29/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.3952\n",
            "Epoch 30/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.3944\n",
            "Epoch 31/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0014 - val_loss: 0.3976\n",
            "Epoch 32/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0013 - val_loss: 0.4037\n",
            "Epoch 33/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0013 - val_loss: 0.4005\n",
            "Epoch 34/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012 - val_loss: 0.4063\n",
            "Epoch 35/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0011 - val_loss: 0.4084\n",
            "Epoch 36/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0011 - val_loss: 0.4090\n",
            "Epoch 37/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0010 - val_loss: 0.4141\n",
            "Epoch 38/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 9.8394e-04 - val_loss: 0.4146\n",
            "Epoch 39/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 9.4719e-04 - val_loss: 0.4188\n",
            "Epoch 40/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 8.9832e-04 - val_loss: 0.4164\n",
            "Epoch 41/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 8.9335e-04 - val_loss: 0.4199\n",
            "Epoch 42/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 8.5278e-04 - val_loss: 0.4209\n",
            "Epoch 43/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 8.1470e-04 - val_loss: 0.4244\n",
            "Epoch 44/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.9092e-04 - val_loss: 0.4255\n",
            "Epoch 45/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.5969e-04 - val_loss: 0.4255\n",
            "Epoch 46/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.3711e-04 - val_loss: 0.4287\n",
            "Epoch 47/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.2477e-04 - val_loss: 0.4314\n",
            "Epoch 48/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.0279e-04 - val_loss: 0.4321\n",
            "Epoch 49/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.7879e-04 - val_loss: 0.4317\n",
            "Epoch 50/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.5359e-04 - val_loss: 0.4357\n",
            "Epoch 51/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.3789e-04 - val_loss: 0.4352\n",
            "Epoch 52/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.1612e-04 - val_loss: 0.4361\n",
            "Epoch 53/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.9916e-04 - val_loss: 0.4375\n",
            "Epoch 54/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.8881e-04 - val_loss: 0.4402\n",
            "Epoch 55/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.7114e-04 - val_loss: 0.4399\n",
            "Epoch 56/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.5904e-04 - val_loss: 0.4431\n",
            "Epoch 57/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.4304e-04 - val_loss: 0.4435\n",
            "Epoch 58/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.3335e-04 - val_loss: 0.4439\n",
            "Epoch 59/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.1724e-04 - val_loss: 0.4471\n",
            "Epoch 60/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.0327e-04 - val_loss: 0.4460\n",
            "Epoch 61/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.9439e-04 - val_loss: 0.4498\n",
            "Epoch 62/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.8361e-04 - val_loss: 0.4499\n",
            "Epoch 63/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.7269e-04 - val_loss: 0.4479\n",
            "Epoch 64/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.6350e-04 - val_loss: 0.4511\n",
            "Epoch 65/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.4910e-04 - val_loss: 0.4519\n",
            "Epoch 66/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.4060e-04 - val_loss: 0.4527\n",
            "Epoch 67/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.3957e-04 - val_loss: 0.4530\n",
            "Epoch 68/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.2527e-04 - val_loss: 0.4557\n",
            "Epoch 69/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 4.1727e-04 - val_loss: 0.4561\n",
            "Epoch 70/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.1118e-04 - val_loss: 0.4568\n",
            "Epoch 71/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.0188e-04 - val_loss: 0.4573\n",
            "Epoch 72/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.9123e-04 - val_loss: 0.4587\n",
            "Epoch 73/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.8796e-04 - val_loss: 0.4598\n",
            "Epoch 74/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.8340e-04 - val_loss: 0.4619\n",
            "Epoch 75/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7160e-04 - val_loss: 0.4594\n",
            "Epoch 76/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.6491e-04 - val_loss: 0.4613\n",
            "Epoch 77/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.5960e-04 - val_loss: 0.4634\n",
            "Epoch 78/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.5558e-04 - val_loss: 0.4646\n",
            "Epoch 79/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.5112e-04 - val_loss: 0.4635\n",
            "Epoch 80/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.4192e-04 - val_loss: 0.4652\n",
            "Epoch 81/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.3595e-04 - val_loss: 0.4653\n",
            "Epoch 82/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.3238e-04 - val_loss: 0.4663\n",
            "Epoch 83/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.2644e-04 - val_loss: 0.4688\n",
            "Epoch 84/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.2017e-04 - val_loss: 0.4676\n",
            "Epoch 85/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.1370e-04 - val_loss: 0.4689\n",
            "Epoch 86/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.1237e-04 - val_loss: 0.4697\n",
            "Epoch 87/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 3.0669e-04 - val_loss: 0.4702\n",
            "Epoch 88/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.0341e-04 - val_loss: 0.4702\n",
            "Epoch 89/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.9670e-04 - val_loss: 0.4713\n",
            "Epoch 90/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.9108e-04 - val_loss: 0.4726\n",
            "Epoch 91/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.8843e-04 - val_loss: 0.4732\n",
            "Epoch 92/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.8362e-04 - val_loss: 0.4733\n",
            "Epoch 93/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.8068e-04 - val_loss: 0.4748\n",
            "Epoch 94/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.7783e-04 - val_loss: 0.4748\n",
            "Epoch 95/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.7287e-04 - val_loss: 0.4757\n",
            "Epoch 96/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.6933e-04 - val_loss: 0.4775\n",
            "Epoch 97/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.6555e-04 - val_loss: 0.4760\n",
            "Epoch 98/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.6123e-04 - val_loss: 0.4780\n",
            "Epoch 99/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.5845e-04 - val_loss: 0.4783\n",
            "Epoch 100/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.5510e-04 - val_loss: 0.4796\n",
            "Epoch 101/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.5026e-04 - val_loss: 0.4787\n",
            "Epoch 102/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.4906e-04 - val_loss: 0.4800\n",
            "Epoch 103/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.4647e-04 - val_loss: 0.4809\n",
            "Epoch 104/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.4201e-04 - val_loss: 0.4814\n",
            "Epoch 105/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.3826e-04 - val_loss: 0.4818\n",
            "Epoch 106/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.3569e-04 - val_loss: 0.4838\n",
            "Epoch 107/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.3359e-04 - val_loss: 0.4832\n",
            "Epoch 108/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.3129e-04 - val_loss: 0.4833\n",
            "Epoch 109/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.2718e-04 - val_loss: 0.4832\n",
            "Epoch 110/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.2522e-04 - val_loss: 0.4857\n",
            "Epoch 111/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.2289e-04 - val_loss: 0.4848\n",
            "Epoch 112/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.1888e-04 - val_loss: 0.4859\n",
            "Epoch 113/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.1808e-04 - val_loss: 0.4865\n",
            "Epoch 114/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.1519e-04 - val_loss: 0.4869\n",
            "Epoch 115/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.1292e-04 - val_loss: 0.4874\n",
            "Epoch 116/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.0931e-04 - val_loss: 0.4880\n",
            "Epoch 117/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.0740e-04 - val_loss: 0.4886\n",
            "Epoch 118/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.0678e-04 - val_loss: 0.4891\n",
            "Epoch 119/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.0351e-04 - val_loss: 0.4897\n",
            "Epoch 120/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 2.0157e-04 - val_loss: 0.4898\n",
            "Epoch 121/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9880e-04 - val_loss: 0.4907\n",
            "Epoch 122/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9686e-04 - val_loss: 0.4913\n",
            "Epoch 123/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.9480e-04 - val_loss: 0.4904\n",
            "Epoch 124/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.9373e-04 - val_loss: 0.4924\n",
            "Epoch 125/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.9106e-04 - val_loss: 0.4934\n",
            "Epoch 126/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.8874e-04 - val_loss: 0.4930\n",
            "Epoch 127/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.8716e-04 - val_loss: 0.4927\n",
            "Epoch 128/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.8544e-04 - val_loss: 0.4932\n",
            "Epoch 129/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.8286e-04 - val_loss: 0.4940\n",
            "Epoch 130/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.8126e-04 - val_loss: 0.4943\n",
            "Epoch 131/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.7996e-04 - val_loss: 0.4955\n",
            "Epoch 132/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.7888e-04 - val_loss: 0.4957\n",
            "Epoch 133/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7622e-04 - val_loss: 0.4966\n",
            "Epoch 134/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.7446e-04 - val_loss: 0.4958\n",
            "Epoch 135/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.7385e-04 - val_loss: 0.4970\n",
            "Epoch 136/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.7222e-04 - val_loss: 0.4971\n",
            "Epoch 137/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.7002e-04 - val_loss: 0.4977\n",
            "Epoch 138/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6828e-04 - val_loss: 0.4981\n",
            "Epoch 139/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.6710e-04 - val_loss: 0.4986\n",
            "Epoch 140/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6549e-04 - val_loss: 0.4990\n",
            "Epoch 141/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.6404e-04 - val_loss: 0.5002\n",
            "Epoch 142/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.6275e-04 - val_loss: 0.5005\n",
            "Epoch 143/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.6082e-04 - val_loss: 0.5006\n",
            "Epoch 144/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.5958e-04 - val_loss: 0.5010\n",
            "Epoch 145/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5839e-04 - val_loss: 0.5019\n",
            "Epoch 146/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5692e-04 - val_loss: 0.5011\n",
            "Epoch 147/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5575e-04 - val_loss: 0.5015\n",
            "Epoch 148/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5417e-04 - val_loss: 0.5033\n",
            "Epoch 149/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.5296e-04 - val_loss: 0.5026\n",
            "Epoch 150/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5159e-04 - val_loss: 0.5038\n",
            "Accuracy: 0.925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "uqpp-Gz9wS8s",
        "outputId": "0f8bfdfa-3a1d-43d8-80f1-ad59c4bc1fa3"
      },
      "source": [
        "# Visualize loss history\n",
        "training_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ7qht9hwS8s",
        "outputId": "e5135615-8da2-4b4b-e085-9c43d7b888b1"
      },
      "source": [
        "Optimal_Number = test_loss.index(min(test_loss)) + 1\n",
        "print(Optimal_Number)\n",
        "print(min(test_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "0.3082735240459442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiitnXPEwS8s"
      },
      "source": [
        "## What is the optimal number of epochs for minimizing the test set loss? \n",
        "<span style=\"color:red\">Answer: As we can see from above, the optimal number of epochs for minimizing the test set loss is 17 (0.3082735240459442)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPBs4-2wS8s"
      },
      "source": [
        "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
        "* *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
        "* Answer a)\n",
        "* <span style=\"color:red\"> I select 1)ADAdelta 2)RMSprop 3)Adam </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V6wu5_8dmso"
      },
      "source": [
        "**1 Adadelta Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_zJeByvwS8s",
        "outputId": "83d2fb84-bb7b-4228-c048-ea13beb9ab67"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def create_model(learning_rate=0.001,rho=0.95,epsilon=1e-6):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    optimizer = Adadelta(learning_rate==learning_rate, rho=rho, epsilon=epsilon)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "    return model\n",
        "learning_rate=[0.001,0.01,0.1]\n",
        "rho=[0.95,0.9,0.85]\n",
        "epsilon = [1e-6,1e-7]\n",
        "param_grid = dict(learning_rate=learning_rate,rho=rho,epsilon=epsilon) \n",
        "\n",
        "# Train the model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=32,verbose=0)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.934992 using {'epsilon': 1e-06, 'learning_rate': 0.1, 'rho': 0.95}\n",
            "0.926229 (0.013922) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'rho': 0.95}\n",
            "0.921240 (0.010671) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'rho': 0.9}\n",
            "0.917486 (0.015974) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'rho': 0.85}\n",
            "0.925000 (0.015291) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'rho': 0.95}\n",
            "0.928736 (0.015964) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'rho': 0.9}\n",
            "0.926229 (0.012507) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'rho': 0.85}\n",
            "0.934992 (0.006454) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'rho': 0.95}\n",
            "0.929975 (0.018806) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'rho': 0.9}\n",
            "0.921245 (0.008140) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'rho': 0.85}\n",
            "0.913731 (0.014150) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'rho': 0.95}\n",
            "0.912473 (0.020504) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'rho': 0.9}\n",
            "0.916233 (0.019508) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'rho': 0.85}\n",
            "0.921231 (0.014140) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'rho': 0.95}\n",
            "0.922494 (0.012773) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'rho': 0.9}\n",
            "0.909995 (0.023107) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'rho': 0.85}\n",
            "0.923742 (0.008902) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'rho': 0.95}\n",
            "0.913736 (0.018676) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'rho': 0.9}\n",
            "0.914998 (0.013800) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'rho': 0.85}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK-T5u_GdtDR"
      },
      "source": [
        "**2 RMSprop Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxsHY8_7GV3",
        "outputId": "f581e723-4e03-4d5b-e340-c2bdecce1fe8"
      },
      "source": [
        "def create_model(learning_rate=0.001,rho=0.95,epsilon=1e-6,momentum=0.0):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    optimizer =RMSprop(learning_rate=learning_rate,rho=rho,momentum=momentum,epsilon=epsilon)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "    return model\n",
        "learning_rate=[0.001,0.01,0.1]\n",
        "rho=[0.95,0.9]\n",
        "epsilon = [1e-6,1e-7]\n",
        "momentum=[0.0,0.1]\n",
        "param_grid = dict(learning_rate=learning_rate,rho=rho,epsilon=epsilon,momentum=momentum) \n",
        "\n",
        "# Train the model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=32,verbose=0)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.943731 using {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.923737 (0.016907) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.924972 (0.017188) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.919987 (0.011675) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.923742 (0.011634) with: {'epsilon': 1e-06, 'learning_rate': 0.001, 'momentum': 0.1, 'rho': 0.9}\n",
            "0.926215 (0.023104) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.943731 (0.010719) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.934997 (0.012377) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.938733 (0.019488) with: {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.1, 'rho': 0.9}\n",
            "0.091258 (0.008915) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.111266 (0.010920) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.304225 (0.269667) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.102494 (0.006978) with: {'epsilon': 1e-06, 'learning_rate': 0.1, 'momentum': 0.1, 'rho': 0.9}\n",
            "0.913736 (0.015977) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.917490 (0.013394) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.922494 (0.024739) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.921250 (0.015291) with: {'epsilon': 1e-07, 'learning_rate': 0.001, 'momentum': 0.1, 'rho': 0.9}\n",
            "0.927487 (0.016904) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.941244 (0.009871) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.922494 (0.012773) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.934992 (0.008892) with: {'epsilon': 1e-07, 'learning_rate': 0.01, 'momentum': 0.1, 'rho': 0.9}\n",
            "0.366877 (0.355291) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'momentum': 0.0, 'rho': 0.95}\n",
            "0.121230 (0.015557) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'momentum': 0.0, 'rho': 0.9}\n",
            "0.121254 (0.018455) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'momentum': 0.1, 'rho': 0.95}\n",
            "0.121254 (0.018455) with: {'epsilon': 1e-07, 'learning_rate': 0.1, 'momentum': 0.1, 'rho': 0.9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmgJrh9wd864"
      },
      "source": [
        "**3 Adam Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGCA9khWbabN",
        "outputId": "fcc86dbe-4635-4783-f64f-aa41b05e3260"
      },
      "source": [
        "def create_model(learning_rate=0.001,epsilon=1e-6):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    optimizer =Adam(learning_rate=learning_rate,epsilon=epsilon)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "    return model\n",
        "learning_rate=[0.001,0.01,0.1]\n",
        "epsilon = [1e-6,1e-7]\n",
        "param_grid = dict(learning_rate=learning_rate,epsilon=epsilon) \n",
        "\n",
        "# Train the model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=32,verbose=0)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.941244 using {'epsilon': 1e-06, 'learning_rate': 0.01}\n",
            "0.912478 (0.013956) with: {'epsilon': 1e-06, 'learning_rate': 0.001}\n",
            "0.941244 (0.012763) with: {'epsilon': 1e-06, 'learning_rate': 0.01}\n",
            "0.111266 (0.010920) with: {'epsilon': 1e-06, 'learning_rate': 0.1}\n",
            "0.916256 (0.006994) with: {'epsilon': 1e-07, 'learning_rate': 0.001}\n",
            "0.939990 (0.013378) with: {'epsilon': 1e-07, 'learning_rate': 0.01}\n",
            "0.117508 (0.004879) with: {'epsilon': 1e-07, 'learning_rate': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay1zWNeqCATY"
      },
      "source": [
        "### 3 Optimizers Summary\n",
        "\n",
        "<span style=\"color:red\"> \n",
        "*Answer b) \n",
        "\n",
        "*   Best for RMSprop 0.943731 using {'epsilon': 1e-06, 'learning_rate': 0.01, 'momentum': 0.0, 'rho': 0.9}\n",
        "*   Best for Adam: 0.941244 using {'epsilon': 1e-06, 'learning_rate': 0.01}\n",
        "\n",
        "*   Best for Adadelta: 0.934992 using {'epsilon': 1e-06, 'learning_rate': 0.1, 'rho': 0.95}\n",
        "\n",
        "\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE6iUN1pCz2J"
      },
      "source": [
        "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rwrz5ryC5he",
        "outputId": "bbdf018f-1d75-490d-afbd-d6624732c8f7"
      },
      "source": [
        "\n",
        "from keras.layers import Dropout\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "def base_model(dropout_rate=0.0):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    # Max pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    #Best optimizer with tuned parameters\n",
        "    optimizer=RMSprop(epsilon=1e-07,learning_rate=0.01,momentum=0.0,rho=0.9)\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='RMSprop',metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=base_model, epochs=150, batch_size=32, verbose=0)\n",
        "# define the grid search parameters\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "param_grid = dict(dropout_rate=dropout_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train,y_oh_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.948725 using {'dropout_rate': 0.7}\n",
            "0.923747 (0.006406) with: {'dropout_rate': 0.0}\n",
            "0.937503 (0.012344) with: {'dropout_rate': 0.1}\n",
            "0.933715 (0.019824) with: {'dropout_rate': 0.2}\n",
            "0.929975 (0.016976) with: {'dropout_rate': 0.3}\n",
            "0.939986 (0.013405) with: {'dropout_rate': 0.4}\n",
            "0.941229 (0.015790) with: {'dropout_rate': 0.5}\n",
            "0.942492 (0.006445) with: {'dropout_rate': 0.6}\n",
            "0.948725 (0.014575) with: {'dropout_rate': 0.7}\n",
            "0.921236 (0.015970) with: {'dropout_rate': 0.8}\n",
            "0.771296 (0.035806) with: {'dropout_rate': 0.9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRc3g9hw6iAe"
      },
      "source": [
        "* Best dropout rate is 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9FVDnTcC4X5"
      },
      "source": [
        "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ23SwuMCuLw",
        "outputId": "950da249-c228-4464-9360-9e57f33a9f03"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history=model.fit(X_train, y_oh_train,validation_data=(X_test,y_oh_test),epochs=150, batch_size=32)\n",
        "# Evaluate performance\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) \n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 2.2656 - accuracy: 0.1975 - val_loss: 2.1583 - val_accuracy: 0.4800\n",
            "Epoch 2/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 1.9508 - accuracy: 0.3200 - val_loss: 1.8882 - val_accuracy: 0.5700\n",
            "Epoch 3/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 1.7414 - accuracy: 0.3950 - val_loss: 1.6205 - val_accuracy: 0.7000\n",
            "Epoch 4/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 1.4435 - accuracy: 0.5412 - val_loss: 1.3567 - val_accuracy: 0.7500\n",
            "Epoch 5/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.3648 - accuracy: 0.5437 - val_loss: 1.2247 - val_accuracy: 0.7850\n",
            "Epoch 6/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.2040 - accuracy: 0.5900 - val_loss: 1.0424 - val_accuracy: 0.7700\n",
            "Epoch 7/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.1303 - accuracy: 0.6125 - val_loss: 0.9668 - val_accuracy: 0.7700\n",
            "Epoch 8/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.0677 - accuracy: 0.6575 - val_loss: 0.8438 - val_accuracy: 0.8200\n",
            "Epoch 9/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.9703 - accuracy: 0.6850 - val_loss: 0.7799 - val_accuracy: 0.8050\n",
            "Epoch 10/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.8988 - accuracy: 0.7038 - val_loss: 0.7186 - val_accuracy: 0.8250\n",
            "Epoch 11/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.8679 - accuracy: 0.7163 - val_loss: 0.6933 - val_accuracy: 0.8650\n",
            "Epoch 12/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.8000 - accuracy: 0.7412 - val_loss: 0.6570 - val_accuracy: 0.8400\n",
            "Epoch 13/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.8139 - accuracy: 0.7337 - val_loss: 0.5643 - val_accuracy: 0.8650\n",
            "Epoch 14/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.7926 - accuracy: 0.7425 - val_loss: 0.5742 - val_accuracy: 0.8450\n",
            "Epoch 15/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.7661 - accuracy: 0.7588 - val_loss: 0.5350 - val_accuracy: 0.8700\n",
            "Epoch 16/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.6854 - accuracy: 0.7925 - val_loss: 0.4915 - val_accuracy: 0.8800\n",
            "Epoch 17/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.7175 - accuracy: 0.7763 - val_loss: 0.5022 - val_accuracy: 0.8950\n",
            "Epoch 18/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.6465 - accuracy: 0.8000 - val_loss: 0.4730 - val_accuracy: 0.8950\n",
            "Epoch 19/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.6117 - accuracy: 0.8037 - val_loss: 0.4227 - val_accuracy: 0.9000\n",
            "Epoch 20/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.6461 - accuracy: 0.8062 - val_loss: 0.3968 - val_accuracy: 0.9150\n",
            "Epoch 21/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.5913 - accuracy: 0.8050 - val_loss: 0.3757 - val_accuracy: 0.9300\n",
            "Epoch 22/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.5593 - accuracy: 0.8200 - val_loss: 0.3881 - val_accuracy: 0.9150\n",
            "Epoch 23/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.5124 - accuracy: 0.8363 - val_loss: 0.3391 - val_accuracy: 0.9300\n",
            "Epoch 24/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.5273 - accuracy: 0.8300 - val_loss: 0.3457 - val_accuracy: 0.9300\n",
            "Epoch 25/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.5155 - accuracy: 0.8537 - val_loss: 0.3510 - val_accuracy: 0.9250\n",
            "Epoch 26/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.4652 - accuracy: 0.8388 - val_loss: 0.3127 - val_accuracy: 0.9300\n",
            "Epoch 27/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.4892 - accuracy: 0.8662 - val_loss: 0.3049 - val_accuracy: 0.9300\n",
            "Epoch 28/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.4394 - accuracy: 0.8650 - val_loss: 0.2978 - val_accuracy: 0.9250\n",
            "Epoch 29/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.4273 - accuracy: 0.8763 - val_loss: 0.2919 - val_accuracy: 0.9300\n",
            "Epoch 30/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.4287 - accuracy: 0.8725 - val_loss: 0.2827 - val_accuracy: 0.9350\n",
            "Epoch 31/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.4151 - accuracy: 0.8687 - val_loss: 0.2693 - val_accuracy: 0.9300\n",
            "Epoch 32/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.3829 - accuracy: 0.8863 - val_loss: 0.2644 - val_accuracy: 0.9350\n",
            "Epoch 33/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3546 - accuracy: 0.8988 - val_loss: 0.2526 - val_accuracy: 0.9400\n",
            "Epoch 34/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3750 - accuracy: 0.8813 - val_loss: 0.2635 - val_accuracy: 0.9200\n",
            "Epoch 35/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3892 - accuracy: 0.8712 - val_loss: 0.2765 - val_accuracy: 0.9250\n",
            "Epoch 36/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3496 - accuracy: 0.8950 - val_loss: 0.2484 - val_accuracy: 0.9350\n",
            "Epoch 37/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3592 - accuracy: 0.8875 - val_loss: 0.2395 - val_accuracy: 0.9350\n",
            "Epoch 38/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3379 - accuracy: 0.8838 - val_loss: 0.2463 - val_accuracy: 0.9200\n",
            "Epoch 39/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.3061 - accuracy: 0.8988 - val_loss: 0.2297 - val_accuracy: 0.9350\n",
            "Epoch 40/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3305 - accuracy: 0.8925 - val_loss: 0.2315 - val_accuracy: 0.9300\n",
            "Epoch 41/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2916 - accuracy: 0.9100 - val_loss: 0.2408 - val_accuracy: 0.9350\n",
            "Epoch 42/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2887 - accuracy: 0.9162 - val_loss: 0.2289 - val_accuracy: 0.9300\n",
            "Epoch 43/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2889 - accuracy: 0.9062 - val_loss: 0.2264 - val_accuracy: 0.9300\n",
            "Epoch 44/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.3125 - accuracy: 0.8975 - val_loss: 0.2353 - val_accuracy: 0.9350\n",
            "Epoch 45/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2710 - accuracy: 0.9100 - val_loss: 0.2175 - val_accuracy: 0.9350\n",
            "Epoch 46/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2979 - accuracy: 0.8963 - val_loss: 0.2184 - val_accuracy: 0.9350\n",
            "Epoch 47/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2897 - accuracy: 0.9000 - val_loss: 0.2135 - val_accuracy: 0.9500\n",
            "Epoch 48/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2419 - accuracy: 0.9175 - val_loss: 0.2083 - val_accuracy: 0.9400\n",
            "Epoch 49/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2928 - accuracy: 0.9162 - val_loss: 0.2251 - val_accuracy: 0.9300\n",
            "Epoch 50/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2556 - accuracy: 0.9200 - val_loss: 0.2183 - val_accuracy: 0.9300\n",
            "Epoch 51/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2642 - accuracy: 0.9100 - val_loss: 0.2073 - val_accuracy: 0.9400\n",
            "Epoch 52/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2678 - accuracy: 0.9212 - val_loss: 0.2069 - val_accuracy: 0.9400\n",
            "Epoch 53/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2470 - accuracy: 0.9250 - val_loss: 0.2008 - val_accuracy: 0.9350\n",
            "Epoch 54/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2317 - accuracy: 0.9287 - val_loss: 0.2068 - val_accuracy: 0.9400\n",
            "Epoch 55/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2680 - accuracy: 0.9150 - val_loss: 0.2027 - val_accuracy: 0.9300\n",
            "Epoch 56/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2386 - accuracy: 0.9300 - val_loss: 0.2045 - val_accuracy: 0.9350\n",
            "Epoch 57/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.2466 - accuracy: 0.9212 - val_loss: 0.1948 - val_accuracy: 0.9400\n",
            "Epoch 58/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2034 - accuracy: 0.9375 - val_loss: 0.1954 - val_accuracy: 0.9350\n",
            "Epoch 59/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2349 - accuracy: 0.9225 - val_loss: 0.1978 - val_accuracy: 0.9400\n",
            "Epoch 60/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2204 - accuracy: 0.9237 - val_loss: 0.2019 - val_accuracy: 0.9400\n",
            "Epoch 61/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2335 - accuracy: 0.9187 - val_loss: 0.1914 - val_accuracy: 0.9350\n",
            "Epoch 62/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2187 - accuracy: 0.9463 - val_loss: 0.1866 - val_accuracy: 0.9350\n",
            "Epoch 63/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2254 - accuracy: 0.9262 - val_loss: 0.1944 - val_accuracy: 0.9350\n",
            "Epoch 64/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2387 - accuracy: 0.9275 - val_loss: 0.1921 - val_accuracy: 0.9350\n",
            "Epoch 65/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2563 - accuracy: 0.9250 - val_loss: 0.1970 - val_accuracy: 0.9350\n",
            "Epoch 66/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2020 - accuracy: 0.9513 - val_loss: 0.1877 - val_accuracy: 0.9450\n",
            "Epoch 67/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2041 - accuracy: 0.9450 - val_loss: 0.1971 - val_accuracy: 0.9350\n",
            "Epoch 68/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1998 - accuracy: 0.9312 - val_loss: 0.1920 - val_accuracy: 0.9450\n",
            "Epoch 69/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1841 - accuracy: 0.9488 - val_loss: 0.1997 - val_accuracy: 0.9450\n",
            "Epoch 70/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1835 - accuracy: 0.9400 - val_loss: 0.1931 - val_accuracy: 0.9400\n",
            "Epoch 71/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2172 - accuracy: 0.9300 - val_loss: 0.1943 - val_accuracy: 0.9350\n",
            "Epoch 72/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1979 - accuracy: 0.9388 - val_loss: 0.1904 - val_accuracy: 0.9400\n",
            "Epoch 73/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2187 - accuracy: 0.9350 - val_loss: 0.1900 - val_accuracy: 0.9400\n",
            "Epoch 74/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2024 - accuracy: 0.9300 - val_loss: 0.1845 - val_accuracy: 0.9400\n",
            "Epoch 75/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1893 - accuracy: 0.9362 - val_loss: 0.1851 - val_accuracy: 0.9400\n",
            "Epoch 76/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1888 - accuracy: 0.9337 - val_loss: 0.1990 - val_accuracy: 0.9400\n",
            "Epoch 77/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2170 - accuracy: 0.9425 - val_loss: 0.2129 - val_accuracy: 0.9450\n",
            "Epoch 78/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1766 - accuracy: 0.9463 - val_loss: 0.1962 - val_accuracy: 0.9500\n",
            "Epoch 79/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1689 - accuracy: 0.9413 - val_loss: 0.1987 - val_accuracy: 0.9400\n",
            "Epoch 80/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1890 - accuracy: 0.9350 - val_loss: 0.2186 - val_accuracy: 0.9300\n",
            "Epoch 81/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1552 - accuracy: 0.9475 - val_loss: 0.1990 - val_accuracy: 0.9400\n",
            "Epoch 82/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1588 - accuracy: 0.9438 - val_loss: 0.1939 - val_accuracy: 0.9450\n",
            "Epoch 83/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.2090 - accuracy: 0.9325 - val_loss: 0.1898 - val_accuracy: 0.9450\n",
            "Epoch 84/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1715 - accuracy: 0.9413 - val_loss: 0.1945 - val_accuracy: 0.9400\n",
            "Epoch 85/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1461 - accuracy: 0.9488 - val_loss: 0.1923 - val_accuracy: 0.9450\n",
            "Epoch 86/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1794 - accuracy: 0.9388 - val_loss: 0.1798 - val_accuracy: 0.9450\n",
            "Epoch 87/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1658 - accuracy: 0.9425 - val_loss: 0.1998 - val_accuracy: 0.9400\n",
            "Epoch 88/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1906 - accuracy: 0.9388 - val_loss: 0.1829 - val_accuracy: 0.9400\n",
            "Epoch 89/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1846 - accuracy: 0.9438 - val_loss: 0.1794 - val_accuracy: 0.9400\n",
            "Epoch 90/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1883 - accuracy: 0.9413 - val_loss: 0.1879 - val_accuracy: 0.9400\n",
            "Epoch 91/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1777 - accuracy: 0.9388 - val_loss: 0.1913 - val_accuracy: 0.9400\n",
            "Epoch 92/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1416 - accuracy: 0.9500 - val_loss: 0.1907 - val_accuracy: 0.9400\n",
            "Epoch 93/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1599 - accuracy: 0.9525 - val_loss: 0.1888 - val_accuracy: 0.9400\n",
            "Epoch 94/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1494 - accuracy: 0.9475 - val_loss: 0.1814 - val_accuracy: 0.9450\n",
            "Epoch 95/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1585 - accuracy: 0.9538 - val_loss: 0.1948 - val_accuracy: 0.9500\n",
            "Epoch 96/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1502 - accuracy: 0.9525 - val_loss: 0.1973 - val_accuracy: 0.9400\n",
            "Epoch 97/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1909 - accuracy: 0.9375 - val_loss: 0.1957 - val_accuracy: 0.9450\n",
            "Epoch 98/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1654 - accuracy: 0.9513 - val_loss: 0.1929 - val_accuracy: 0.9450\n",
            "Epoch 99/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1454 - accuracy: 0.9425 - val_loss: 0.1937 - val_accuracy: 0.9450\n",
            "Epoch 100/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1425 - accuracy: 0.9550 - val_loss: 0.2022 - val_accuracy: 0.9500\n",
            "Epoch 101/150\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1442 - accuracy: 0.9475 - val_loss: 0.1976 - val_accuracy: 0.9400\n",
            "Epoch 102/150\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 0.1530 - accuracy: 0.9513 - val_loss: 0.1887 - val_accuracy: 0.9450\n",
            "Epoch 103/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1509 - accuracy: 0.9513 - val_loss: 0.1825 - val_accuracy: 0.9450\n",
            "Epoch 104/150\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1654 - accuracy: 0.9575 - val_loss: 0.1801 - val_accuracy: 0.9450\n",
            "Epoch 105/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1371 - accuracy: 0.9550 - val_loss: 0.1921 - val_accuracy: 0.9500\n",
            "Epoch 106/150\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1433 - accuracy: 0.9488 - val_loss: 0.1917 - val_accuracy: 0.9350\n",
            "Epoch 107/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1598 - accuracy: 0.9400 - val_loss: 0.1911 - val_accuracy: 0.9450\n",
            "Epoch 108/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1351 - accuracy: 0.9575 - val_loss: 0.1965 - val_accuracy: 0.9450\n",
            "Epoch 109/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1385 - accuracy: 0.9550 - val_loss: 0.1758 - val_accuracy: 0.9450\n",
            "Epoch 110/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1529 - accuracy: 0.9362 - val_loss: 0.1951 - val_accuracy: 0.9450\n",
            "Epoch 111/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1315 - accuracy: 0.9538 - val_loss: 0.1837 - val_accuracy: 0.9450\n",
            "Epoch 112/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1142 - accuracy: 0.9700 - val_loss: 0.1901 - val_accuracy: 0.9500\n",
            "Epoch 113/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1232 - accuracy: 0.9525 - val_loss: 0.1898 - val_accuracy: 0.9450\n",
            "Epoch 114/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1265 - accuracy: 0.9575 - val_loss: 0.1933 - val_accuracy: 0.9550\n",
            "Epoch 115/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1323 - accuracy: 0.9550 - val_loss: 0.1777 - val_accuracy: 0.9550\n",
            "Epoch 116/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1444 - accuracy: 0.9550 - val_loss: 0.1885 - val_accuracy: 0.9550\n",
            "Epoch 117/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1281 - accuracy: 0.9563 - val_loss: 0.1789 - val_accuracy: 0.9450\n",
            "Epoch 118/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1246 - accuracy: 0.9563 - val_loss: 0.1879 - val_accuracy: 0.9500\n",
            "Epoch 119/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1142 - accuracy: 0.9638 - val_loss: 0.1764 - val_accuracy: 0.9550\n",
            "Epoch 120/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1497 - accuracy: 0.9513 - val_loss: 0.1724 - val_accuracy: 0.9500\n",
            "Epoch 121/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.1781 - val_accuracy: 0.9600\n",
            "Epoch 122/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1382 - accuracy: 0.9550 - val_loss: 0.1739 - val_accuracy: 0.9600\n",
            "Epoch 123/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1319 - accuracy: 0.9575 - val_loss: 0.1777 - val_accuracy: 0.9550\n",
            "Epoch 124/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1221 - accuracy: 0.9563 - val_loss: 0.1820 - val_accuracy: 0.9600\n",
            "Epoch 125/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1101 - accuracy: 0.9638 - val_loss: 0.1841 - val_accuracy: 0.9650\n",
            "Epoch 126/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1191 - accuracy: 0.9688 - val_loss: 0.1753 - val_accuracy: 0.9550\n",
            "Epoch 127/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1232 - accuracy: 0.9638 - val_loss: 0.1763 - val_accuracy: 0.9600\n",
            "Epoch 128/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1417 - accuracy: 0.9475 - val_loss: 0.1863 - val_accuracy: 0.9550\n",
            "Epoch 129/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1086 - accuracy: 0.9663 - val_loss: 0.1836 - val_accuracy: 0.9550\n",
            "Epoch 130/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1423 - accuracy: 0.9525 - val_loss: 0.1837 - val_accuracy: 0.9650\n",
            "Epoch 131/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1298 - accuracy: 0.9625 - val_loss: 0.1857 - val_accuracy: 0.9500\n",
            "Epoch 132/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 0.1898 - val_accuracy: 0.9600\n",
            "Epoch 133/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0976 - accuracy: 0.9650 - val_loss: 0.1830 - val_accuracy: 0.9500\n",
            "Epoch 134/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1331 - accuracy: 0.9600 - val_loss: 0.1808 - val_accuracy: 0.9450\n",
            "Epoch 135/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1332 - accuracy: 0.9513 - val_loss: 0.1815 - val_accuracy: 0.9550\n",
            "Epoch 136/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1128 - accuracy: 0.9513 - val_loss: 0.1866 - val_accuracy: 0.9550\n",
            "Epoch 137/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1361 - accuracy: 0.9613 - val_loss: 0.1679 - val_accuracy: 0.9600\n",
            "Epoch 138/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1286 - accuracy: 0.9563 - val_loss: 0.1808 - val_accuracy: 0.9550\n",
            "Epoch 139/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.0927 - accuracy: 0.9663 - val_loss: 0.1869 - val_accuracy: 0.9450\n",
            "Epoch 140/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1729 - accuracy: 0.9463 - val_loss: 0.1812 - val_accuracy: 0.9650\n",
            "Epoch 141/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1238 - accuracy: 0.9588 - val_loss: 0.1826 - val_accuracy: 0.9500\n",
            "Epoch 142/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1145 - accuracy: 0.9600 - val_loss: 0.1988 - val_accuracy: 0.9500\n",
            "Epoch 143/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1228 - accuracy: 0.9613 - val_loss: 0.1991 - val_accuracy: 0.9550\n",
            "Epoch 144/150\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 0.1269 - accuracy: 0.9588 - val_loss: 0.1758 - val_accuracy: 0.9600\n",
            "Epoch 145/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1045 - accuracy: 0.9613 - val_loss: 0.1937 - val_accuracy: 0.9550\n",
            "Epoch 146/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1052 - accuracy: 0.9650 - val_loss: 0.1793 - val_accuracy: 0.9550\n",
            "Epoch 147/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1529 - accuracy: 0.9575 - val_loss: 0.1809 - val_accuracy: 0.9500\n",
            "Epoch 148/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1126 - accuracy: 0.9625 - val_loss: 0.1912 - val_accuracy: 0.9500\n",
            "Epoch 149/150\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.1248 - accuracy: 0.9588 - val_loss: 0.1939 - val_accuracy: 0.9550\n",
            "Epoch 150/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1401 - accuracy: 0.9600 - val_loss: 0.1979 - val_accuracy: 0.9650\n",
            "Accuracy: 0.965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zBKp1Ai5hEC"
      },
      "source": [
        "<span style=\"color:red\"> Final model accuracy 0.965: RMSprop with dropout(0.7) compared with Adam: 0.941244 ,Adadelta: 0.934992\n",
        "\n",
        "* Average Models Accuracy of 3 optimizer:  0.947 , the final best model accuracy is 0.018 higher than average accuracy\n",
        "\n",
        "* Worst Model: Adam: 0.934992, the final best accuracy is 0.03 higher than the  worst model.\n",
        "\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "KYooe9mp3UOn",
        "outputId": "e183df2f-2a44-440c-8d03-f70f36f15f8a"
      },
      "source": [
        "# Visualize loss history\n",
        "training_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA2Oi8Cq4xwT"
      },
      "source": [
        "* We can see after epoch=80, test loss is greater than training loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "K2JSMEaJ39tt",
        "outputId": "e41be4ae-7295-4d22-e906-449069510ff6"
      },
      "source": [
        "# Visualize accuracy vs val_accuracy history\n",
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "epoch_count = range(1, len(accuracy) + 1)\n",
        "plt.plot(epoch_count, accuracy, 'r--')\n",
        "plt.plot(epoch_count, val_accuracy, 'b-')\n",
        "plt.legend(['Training Accuracy', 'Test set Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMvkzUeZFFa7"
      },
      "source": [
        "In our final model, as we can see from picture, before epoch=80 training accuracy is always greater than test accuracy. Because we have a high dropout rate(0.7 compared with 0.0 previously) which will disable neurons and sample is lost, then subsequent layers attempt to construct the output based on incomplete input.  \n",
        "\n",
        "After epoch reach to 80+, during validation all of the sample are available, so the cnn has its full computational power so the test accuracy may be greater than training accuracy, so test set might perform a little better than training set after full computational power. The loss of test set is higher because it has a smaller size than training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrAN7CddDZ4n",
        "outputId": "5cf50088-6d94-42e5-ccd0-93d70263ba6e"
      },
      "source": [
        "\n",
        "unique1, counts1 = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique1, counts1/len(y_train))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.09625, 1: 0.1075, 2: 0.10375, 3: 0.0925, 4: 0.11125, 5: 0.08625, 6: 0.0875, 7: 0.12125, 8: 0.08875, 9: 0.105}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OiNumIQELQf",
        "outputId": "8750079b-819b-4ca5-ccab-d023b0ac3c76"
      },
      "source": [
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(dict(zip(unique, counts/len(y_test))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.1, 1: 0.15, 2: 0.08, 3: 0.095, 4: 0.08, 5: 0.115, 6: 0.12, 7: 0.1, 8: 0.08, 9: 0.08}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKOU7onPHK8a",
        "outputId": "6e66536c-a334-4410-b8d9-3225f053b5c3"
      },
      "source": [
        "unique2, counts2 = np.unique(y, return_counts=True)\n",
        "print(dict(zip(unique2, counts2/len(y))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.097, 1: 0.116, 2: 0.099, 3: 0.093, 4: 0.105, 5: 0.092, 6: 0.094, 7: 0.117, 8: 0.087, 9: 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9d_7O_swS8s"
      },
      "source": [
        "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
        "\n",
        "    <span style=\"color:red\"> \n",
        "    * Answer: \n",
        "    \n",
        "    * From above figures, we can see training accuracy and test accuracy reach almost the same optimal value, and also test set and train set have close distribution of 10 digits which can reflect the distribution of the whole datasets, so the performance on the test set can representative for the performance one would expect on a previously unseen data set drawn from the same distribution.\n",
        "    * If we want to have more accurate cross-validation test set, we can use StratifiedKFold() to preserve the percentage of samples for each class. </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFOZCmiDwS8s"
      },
      "source": [
        "## Further information\n",
        "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\n",
        "\n",
        "\n",
        "**Good luck!**"
      ]
    }
  ]
}