{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fziWVL1xsUz"
      },
      "source": [
        "## 1. Loads the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP8TGsxFxsU2"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v14teO56V6RF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E0aGoF8xsU2"
      },
      "source": [
        "def load_data(fname): \n",
        "    data = pd.read_csv(fname, index_col=0) \n",
        "    X = data.values[:, 2:].astype('float64') \n",
        "    years = data['year'] \n",
        "    X_train = X[years < 2020.] \n",
        "    X_valid = X[years == 2020.] \n",
        "    tmp = data.index[data['year'] == 2020.] \n",
        "    tickers = np.array([ticker.rstrip('_2020') for ticker in tmp]) \n",
        "    return X_train, X_valid, tickers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPm87ZDMxsU2"
      },
      "source": [
        "X_train, X_valid, tickers= load_data('assign2_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2GUQpnex0qV",
        "outputId": "83ef8810-d5e7-4cdb-f83e-971bcfcc557a"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1396366 , 0.12718708, 0.12752356, ..., 0.17059219, 0.16655451,\n",
              "        0.16722745],\n",
              "       [0.92412051, 0.87924868, 0.86732257, ..., 0.62805603, 0.62969588,\n",
              "        0.63387004],\n",
              "       [0.19620253, 0.1722754 , 0.16625501, ..., 0.44272922, 0.44118555,\n",
              "        0.44350105],\n",
              "       ...,\n",
              "       [0.305288  , 0.30944285, 0.29102928, ..., 0.47365439, 0.47610952,\n",
              "        0.47431539],\n",
              "       [0.47841174, 0.49534703, 0.51170263, ..., 0.82249278, 0.81413264,\n",
              "        0.81944075],\n",
              "       [0.26903134, 0.3054443 , 0.33030879, ..., 0.2512373 , 0.23945319,\n",
              "        0.2389819 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHnoqZW3KWXw",
        "outputId": "351f2798-f8d5-49ca-db5b-61ee6d938656"
      },
      "source": [
        "X_valid.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "litCdc0ixsU3"
      },
      "source": [
        "## 2. Sets up and trains the autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFIzZSkxsU3",
        "outputId": "f368e420-580a-469a-f6c1-f8c3f92984c0"
      },
      "source": [
        "# set the random seed\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# setting parameters\n",
        "input_shape = X_train.shape[1:]\n",
        "input_size = X_train.shape[1]\n",
        "encoding_dim = 5\n",
        "batch_size = 32\n",
        "epoch = 30\n",
        "values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "all_train,all_test =list(),list()\n",
        "\n",
        "\n",
        "# Build the Autoencoder Model with searching for best l2 parameter\n",
        "\n",
        "for param in values:\n",
        "  input_data = keras.Input(shape=input_shape)\n",
        "\n",
        "  #Encode Layer\n",
        "  encoded = layers.Dense(encoding_dim, activation='relu',kernel_regularizer=keras.regularizers.l2(param))(input_data)\n",
        "\n",
        "  #Decode Layer\n",
        "  decoded = layers.Dense(input_size, activation='sigmoid',kernel_regularizer=keras.regularizers.l2(param))(encoded)\n",
        "\n",
        "  #Autoencoder Model\n",
        "  autoencoder = keras.Model(input_data, decoded)\n",
        "  \n",
        "  #Encoder Model (for test purpose)\n",
        "  encoder = keras.Model(input_data, encoded)\n",
        "\n",
        "  # This is our encoded (5-dimensional) input (for test purpose)\n",
        "  encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "\n",
        "  # Retrieve the last layer of the autoencoder model (for test purpose)\n",
        "  decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "  #Decoder Model (for test purpose)\n",
        "  decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "  # Complie and Train the autoencoder\n",
        "  autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "  model = autoencoder.fit(X_train,X_train,\n",
        "                  validation_data=(X_valid, X_valid),\n",
        "                  epochs=epoch,\n",
        "                  batch_size=batch_size)\n",
        "  \n",
        "  # evaluate the model with X_train\n",
        "  # encoded_data = encoder.predict(X_train)\n",
        "  # decoded_data = decoder.predict(encoded_data)\n",
        "  # train_mse = mean_squared_error(X_train, decoded_data)\n",
        "\n",
        "  X_train_output = autoencoder.predict(X_train)\n",
        "  train_mse = mean_squared_error(X_train, X_train_output)\n",
        "\n",
        "\n",
        "  # evaluate the model with X_valid \n",
        "  # encoded_data = encoder.predict(X_valid)\n",
        "  # decoded_data = decoder.predict(encoded_data)\n",
        "  # test_mse = mean_squared_error(X_valid, decoded_data)\n",
        "\n",
        "  X_valid_output = autoencoder.predict(X_valid)\n",
        "  test_mse = mean_squared_error(X_valid, X_valid_output)\n",
        "\n",
        "  # store mse for different parameter for l2\n",
        "  all_test.append(test_mse)\n",
        "  all_train.append(train_mse)\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 16ms/step - loss: 1.8923 - val_loss: 1.5216\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4106 - val_loss: 1.1228\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1.0345 - val_loss: 0.8237\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7559 - val_loss: 0.6038\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5508 - val_loss: 0.4444\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4039 - val_loss: 0.3301\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3018 - val_loss: 0.2492\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2268 - val_loss: 0.1926\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1696 - val_loss: 0.1535\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1415 - val_loss: 0.1268\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1125 - val_loss: 0.1088\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 0.0968\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.0889\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 0.0836\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0801\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0777\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.0760\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0652 - val_loss: 0.0748\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.0739\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0660 - val_loss: 0.0731\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0725\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0719\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0643 - val_loss: 0.0714\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0709\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0608 - val_loss: 0.0705\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0700\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.0696\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0693\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.0689\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0686\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 16ms/step - loss: 0.2629 - val_loss: 0.2325\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2162 - val_loss: 0.1912\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1743 - val_loss: 0.1601\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1450 - val_loss: 0.1371\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1224 - val_loss: 0.1202\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1071 - val_loss: 0.1079\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0990 - val_loss: 0.0989\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0901 - val_loss: 0.0925\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0788 - val_loss: 0.0878\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0821 - val_loss: 0.0844\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.0818\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0799\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.0785\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0711 - val_loss: 0.0773\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0657 - val_loss: 0.0763\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.0755\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0670 - val_loss: 0.0747\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.0741\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0734\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0656 - val_loss: 0.0729\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0723\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0652 - val_loss: 0.0718\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0641 - val_loss: 0.0713\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0709\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.0704\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.0700\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.0696\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0692\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.0689\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0685\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 17ms/step - loss: 0.0994 - val_loss: 0.1032\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0965 - val_loss: 0.0981\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0884 - val_loss: 0.0939\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 0.0907\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0796 - val_loss: 0.0881\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0773 - val_loss: 0.0863\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 0.0848\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.0838\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0829\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0751 - val_loss: 0.0819\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.0807\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0692 - val_loss: 0.0797\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.0788\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0699 - val_loss: 0.0777\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.0771\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.0761\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0655 - val_loss: 0.0744\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.0740\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.0731\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0638 - val_loss: 0.0717\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0711\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0701\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0690\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0565 - val_loss: 0.0685\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.0673\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0664\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0660\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0545 - val_loss: 0.0648\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0554 - val_loss: 0.0636\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0517 - val_loss: 0.0632\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 17ms/step - loss: 0.0831 - val_loss: 0.0903\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 0.0889\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0876\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0862\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.0856\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0843\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0739 - val_loss: 0.0819\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0802\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0647 - val_loss: 0.0790\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0693 - val_loss: 0.0764\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0750\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0610 - val_loss: 0.0733\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0572 - val_loss: 0.0707\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0698\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.0670\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0534 - val_loss: 0.0637\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.0614\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0479 - val_loss: 0.0594\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0482 - val_loss: 0.0574\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.0550\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0422 - val_loss: 0.0544\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0446 - val_loss: 0.0538\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0429 - val_loss: 0.0492\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0502\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0470\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0391 - val_loss: 0.0468\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0379 - val_loss: 0.0448\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0470\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0356 - val_loss: 0.0418\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0332 - val_loss: 0.0419\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 18ms/step - loss: 0.0813 - val_loss: 0.0887\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0876\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0866\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0763 - val_loss: 0.0854\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0846\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0835\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0737 - val_loss: 0.0817\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0800\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.0788\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0693 - val_loss: 0.0757\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0741\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0721\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0705\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0692\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0552 - val_loss: 0.0679\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.0661\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0535 - val_loss: 0.0646\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0516 - val_loss: 0.0638\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0515 - val_loss: 0.0631\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.0614\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0470 - val_loss: 0.0612\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0506 - val_loss: 0.0592\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0476 - val_loss: 0.0585\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.0568\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0437 - val_loss: 0.0563\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0450 - val_loss: 0.0545\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.0543\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0525\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.0515\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.0506\n",
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 16ms/step - loss: 0.0811 - val_loss: 0.0886\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.0876\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0865\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 0.0850\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0843\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0827\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0805\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0788\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0640 - val_loss: 0.0780\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0688 - val_loss: 0.0747\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0735\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.0718\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0576 - val_loss: 0.0701\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0698\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0549 - val_loss: 0.0677\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0543 - val_loss: 0.0655\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0527 - val_loss: 0.0643\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0507 - val_loss: 0.0638\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0505 - val_loss: 0.0624\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0502 - val_loss: 0.0605\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.0605\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0496 - val_loss: 0.0598\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0461 - val_loss: 0.0559\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0431 - val_loss: 0.0573\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.0540\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.0545\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.0525\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0426 - val_loss: 0.0521\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0410 - val_loss: 0.0500\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.0522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "6loVabk4y708",
        "outputId": "5ecd627e-b2f5-4459-fb7c-23f1193e7795"
      },
      "source": [
        "\n",
        "plt.semilogx(values, all_test, label='test', marker='o')\n",
        "plt.semilogx(values, all_train, label='train', marker='x')\n",
        "plt.title('Compare l2 parameter MSE result')\n",
        "plt.xlabel('l2 parameter')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH5fC8v-Dc-Q"
      },
      "source": [
        "The best l2 parameter for X_valid is 1e-4\n",
        "### Retrain the model with best parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuNrmIux9PWs",
        "outputId": "239f3e13-a727-402f-a7a9-ebedb2e9b0b3"
      },
      "source": [
        "\n",
        "#Encode Layer\n",
        "encoded = layers.Dense(encoding_dim, activation='relu',kernel_regularizer=keras.regularizers.l2(1e-4))(input_data)\n",
        "\n",
        "#Decode Layer\n",
        "decoded = layers.Dense(input_size, activation='sigmoid',kernel_regularizer=keras.regularizers.l2(1e-4))(encoded)\n",
        "\n",
        "#Autoencoder Model \n",
        "autoencoder = keras.Model(input_data, decoded)\n",
        "  \n",
        "# Complie and Train the autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(X_train,X_train,\n",
        "                  validation_data=(X_valid, X_valid),\n",
        "                  epochs=epoch,\n",
        "                  batch_size=batch_size)\n",
        "  \n",
        "# evaluate the model\n",
        "X_valid_output = autoencoder.predict(X_valid)\n",
        "test_mse = mean_squared_error(X_valid, X_valid_output)\n",
        "\n",
        "# print mse\n",
        "print('MSE for Validation')\n",
        "print(test_mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15/15 [==============================] - 1s 18ms/step - loss: 0.0830 - val_loss: 0.0903\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0844 - val_loss: 0.0890\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0879\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0864\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0738 - val_loss: 0.0856\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0839\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0740 - val_loss: 0.0812\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0710 - val_loss: 0.0790\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0639 - val_loss: 0.0773\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0684 - val_loss: 0.0744\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0734\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0717\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0574 - val_loss: 0.0707\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0699\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0550 - val_loss: 0.0693\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.0677\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.0667\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.0658\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.0652\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0518 - val_loss: 0.0629\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.0621\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.0584\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.0550\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.0552\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0417 - val_loss: 0.0511\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0418 - val_loss: 0.0508\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0481\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0379 - val_loss: 0.0502\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.0447\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0459\n",
            "MSE for Validation\n",
            "0.04023128454496991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3F1uMC33K9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cb86e4-a84d-4b31-dd30-f620b2df4a2f"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_47 (InputLayer)        [(None, 250)]             0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 5)                 1255      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 250)               1500      \n",
            "=================================================================\n",
            "Total params: 2,755\n",
            "Trainable params: 2,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuORIQQCDsyo"
      },
      "source": [
        "## 3. Selects the 5 most communal and 20 least communal stocks. The ticker symbols for the 2 sets of stocks should be stored in\n",
        "lists called most_communal and least_communal ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "pj7CSIh3Dx2z",
        "outputId": "9e14d9a9-fe0a-460d-af04-5a38387a004b"
      },
      "source": [
        "df = pd.DataFrame(X_valid,index=tickers)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>INO</th>\n",
              "      <td>0.039704</td>\n",
              "      <td>0.039367</td>\n",
              "      <td>0.038022</td>\n",
              "      <td>0.038358</td>\n",
              "      <td>0.045087</td>\n",
              "      <td>0.044415</td>\n",
              "      <td>0.046433</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.044415</td>\n",
              "      <td>0.053499</td>\n",
              "      <td>0.049462</td>\n",
              "      <td>0.062921</td>\n",
              "      <td>0.076380</td>\n",
              "      <td>0.112719</td>\n",
              "      <td>0.080417</td>\n",
              "      <td>0.072005</td>\n",
              "      <td>0.090511</td>\n",
              "      <td>0.086137</td>\n",
              "      <td>0.065276</td>\n",
              "      <td>0.058546</td>\n",
              "      <td>0.053499</td>\n",
              "      <td>0.047106</td>\n",
              "      <td>0.041050</td>\n",
              "      <td>0.050808</td>\n",
              "      <td>0.047779</td>\n",
              "      <td>0.046097</td>\n",
              "      <td>0.063594</td>\n",
              "      <td>0.073351</td>\n",
              "      <td>0.075707</td>\n",
              "      <td>0.061911</td>\n",
              "      <td>0.056191</td>\n",
              "      <td>0.059892</td>\n",
              "      <td>0.066958</td>\n",
              "      <td>0.059556</td>\n",
              "      <td>0.067968</td>\n",
              "      <td>0.080417</td>\n",
              "      <td>0.077725</td>\n",
              "      <td>0.081427</td>\n",
              "      <td>0.184388</td>\n",
              "      <td>0.203903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300808</td>\n",
              "      <td>0.297106</td>\n",
              "      <td>0.288022</td>\n",
              "      <td>0.219717</td>\n",
              "      <td>0.320996</td>\n",
              "      <td>0.294415</td>\n",
              "      <td>0.315949</td>\n",
              "      <td>0.311238</td>\n",
              "      <td>0.366083</td>\n",
              "      <td>0.327389</td>\n",
              "      <td>0.305182</td>\n",
              "      <td>0.304509</td>\n",
              "      <td>0.288358</td>\n",
              "      <td>0.282974</td>\n",
              "      <td>0.275236</td>\n",
              "      <td>0.283311</td>\n",
              "      <td>0.346231</td>\n",
              "      <td>0.344886</td>\n",
              "      <td>0.351615</td>\n",
              "      <td>0.336474</td>\n",
              "      <td>0.354980</td>\n",
              "      <td>0.357672</td>\n",
              "      <td>0.343203</td>\n",
              "      <td>0.335801</td>\n",
              "      <td>0.318304</td>\n",
              "      <td>0.315949</td>\n",
              "      <td>0.305855</td>\n",
              "      <td>0.283984</td>\n",
              "      <td>0.301144</td>\n",
              "      <td>0.284993</td>\n",
              "      <td>0.294415</td>\n",
              "      <td>0.283311</td>\n",
              "      <td>0.281292</td>\n",
              "      <td>0.266824</td>\n",
              "      <td>0.272544</td>\n",
              "      <td>0.278937</td>\n",
              "      <td>0.256729</td>\n",
              "      <td>0.233849</td>\n",
              "      <td>0.236541</td>\n",
              "      <td>0.231494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALKS</th>\n",
              "      <td>0.090340</td>\n",
              "      <td>0.092874</td>\n",
              "      <td>0.091682</td>\n",
              "      <td>0.096303</td>\n",
              "      <td>0.088849</td>\n",
              "      <td>0.095259</td>\n",
              "      <td>0.103459</td>\n",
              "      <td>0.103309</td>\n",
              "      <td>0.092129</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.082290</td>\n",
              "      <td>0.076625</td>\n",
              "      <td>0.067979</td>\n",
              "      <td>0.060972</td>\n",
              "      <td>0.070215</td>\n",
              "      <td>0.070811</td>\n",
              "      <td>0.065593</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.063655</td>\n",
              "      <td>0.066488</td>\n",
              "      <td>0.094514</td>\n",
              "      <td>0.090489</td>\n",
              "      <td>0.087955</td>\n",
              "      <td>0.090042</td>\n",
              "      <td>0.090042</td>\n",
              "      <td>0.090042</td>\n",
              "      <td>0.068426</td>\n",
              "      <td>0.070215</td>\n",
              "      <td>0.067829</td>\n",
              "      <td>0.083631</td>\n",
              "      <td>0.109869</td>\n",
              "      <td>0.107036</td>\n",
              "      <td>0.100030</td>\n",
              "      <td>0.102415</td>\n",
              "      <td>0.107335</td>\n",
              "      <td>0.097943</td>\n",
              "      <td>0.115981</td>\n",
              "      <td>0.121944</td>\n",
              "      <td>0.104353</td>\n",
              "      <td>0.108527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065444</td>\n",
              "      <td>0.060674</td>\n",
              "      <td>0.058736</td>\n",
              "      <td>0.072302</td>\n",
              "      <td>0.074836</td>\n",
              "      <td>0.067829</td>\n",
              "      <td>0.063506</td>\n",
              "      <td>0.070662</td>\n",
              "      <td>0.071258</td>\n",
              "      <td>0.078116</td>\n",
              "      <td>0.074240</td>\n",
              "      <td>0.076178</td>\n",
              "      <td>0.077221</td>\n",
              "      <td>0.077072</td>\n",
              "      <td>0.079159</td>\n",
              "      <td>0.080054</td>\n",
              "      <td>0.081097</td>\n",
              "      <td>0.077519</td>\n",
              "      <td>0.080501</td>\n",
              "      <td>0.085868</td>\n",
              "      <td>0.088551</td>\n",
              "      <td>0.089744</td>\n",
              "      <td>0.084526</td>\n",
              "      <td>0.084824</td>\n",
              "      <td>0.078861</td>\n",
              "      <td>0.113596</td>\n",
              "      <td>0.122838</td>\n",
              "      <td>0.131485</td>\n",
              "      <td>0.134764</td>\n",
              "      <td>0.125820</td>\n",
              "      <td>0.128354</td>\n",
              "      <td>0.133274</td>\n",
              "      <td>0.130292</td>\n",
              "      <td>0.125522</td>\n",
              "      <td>0.122242</td>\n",
              "      <td>0.125224</td>\n",
              "      <td>0.122093</td>\n",
              "      <td>0.114788</td>\n",
              "      <td>0.110167</td>\n",
              "      <td>0.102713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AERI</th>\n",
              "      <td>0.246064</td>\n",
              "      <td>0.238345</td>\n",
              "      <td>0.252393</td>\n",
              "      <td>0.233251</td>\n",
              "      <td>0.243131</td>\n",
              "      <td>0.253165</td>\n",
              "      <td>0.237882</td>\n",
              "      <td>0.231399</td>\n",
              "      <td>0.210250</td>\n",
              "      <td>0.207471</td>\n",
              "      <td>0.203149</td>\n",
              "      <td>0.207471</td>\n",
              "      <td>0.191108</td>\n",
              "      <td>0.182464</td>\n",
              "      <td>0.191571</td>\n",
              "      <td>0.186169</td>\n",
              "      <td>0.184779</td>\n",
              "      <td>0.175363</td>\n",
              "      <td>0.178450</td>\n",
              "      <td>0.175672</td>\n",
              "      <td>0.179685</td>\n",
              "      <td>0.186169</td>\n",
              "      <td>0.175517</td>\n",
              "      <td>0.175672</td>\n",
              "      <td>0.173973</td>\n",
              "      <td>0.165020</td>\n",
              "      <td>0.162705</td>\n",
              "      <td>0.159463</td>\n",
              "      <td>0.152671</td>\n",
              "      <td>0.159772</td>\n",
              "      <td>0.166255</td>\n",
              "      <td>0.192498</td>\n",
              "      <td>0.172584</td>\n",
              "      <td>0.148503</td>\n",
              "      <td>0.148348</td>\n",
              "      <td>0.121797</td>\n",
              "      <td>0.129361</td>\n",
              "      <td>0.135227</td>\n",
              "      <td>0.121797</td>\n",
              "      <td>0.130441</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027323</td>\n",
              "      <td>0.024853</td>\n",
              "      <td>0.022229</td>\n",
              "      <td>0.050247</td>\n",
              "      <td>0.058814</td>\n",
              "      <td>0.060667</td>\n",
              "      <td>0.059895</td>\n",
              "      <td>0.059586</td>\n",
              "      <td>0.059123</td>\n",
              "      <td>0.060204</td>\n",
              "      <td>0.054569</td>\n",
              "      <td>0.050170</td>\n",
              "      <td>0.045153</td>\n",
              "      <td>0.048472</td>\n",
              "      <td>0.053103</td>\n",
              "      <td>0.055881</td>\n",
              "      <td>0.059895</td>\n",
              "      <td>0.050787</td>\n",
              "      <td>0.050479</td>\n",
              "      <td>0.056036</td>\n",
              "      <td>0.054492</td>\n",
              "      <td>0.056345</td>\n",
              "      <td>0.056345</td>\n",
              "      <td>0.058043</td>\n",
              "      <td>0.045230</td>\n",
              "      <td>0.048626</td>\n",
              "      <td>0.050787</td>\n",
              "      <td>0.053412</td>\n",
              "      <td>0.059586</td>\n",
              "      <td>0.057425</td>\n",
              "      <td>0.058197</td>\n",
              "      <td>0.062674</td>\n",
              "      <td>0.061747</td>\n",
              "      <td>0.081507</td>\n",
              "      <td>0.082896</td>\n",
              "      <td>0.080426</td>\n",
              "      <td>0.076258</td>\n",
              "      <td>0.065298</td>\n",
              "      <td>0.069003</td>\n",
              "      <td>0.067768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TXMD</th>\n",
              "      <td>0.137216</td>\n",
              "      <td>0.142152</td>\n",
              "      <td>0.141165</td>\n",
              "      <td>0.145114</td>\n",
              "      <td>0.148075</td>\n",
              "      <td>0.155972</td>\n",
              "      <td>0.165844</td>\n",
              "      <td>0.149062</td>\n",
              "      <td>0.142152</td>\n",
              "      <td>0.191510</td>\n",
              "      <td>0.180652</td>\n",
              "      <td>0.166831</td>\n",
              "      <td>0.158934</td>\n",
              "      <td>0.161895</td>\n",
              "      <td>0.162883</td>\n",
              "      <td>0.155972</td>\n",
              "      <td>0.148075</td>\n",
              "      <td>0.136229</td>\n",
              "      <td>0.137216</td>\n",
              "      <td>0.142152</td>\n",
              "      <td>0.167818</td>\n",
              "      <td>0.153011</td>\n",
              "      <td>0.145114</td>\n",
              "      <td>0.155972</td>\n",
              "      <td>0.169793</td>\n",
              "      <td>0.160908</td>\n",
              "      <td>0.154985</td>\n",
              "      <td>0.158934</td>\n",
              "      <td>0.153011</td>\n",
              "      <td>0.151037</td>\n",
              "      <td>0.126357</td>\n",
              "      <td>0.117473</td>\n",
              "      <td>0.101678</td>\n",
              "      <td>0.096742</td>\n",
              "      <td>0.089832</td>\n",
              "      <td>0.077986</td>\n",
              "      <td>0.080948</td>\n",
              "      <td>0.075025</td>\n",
              "      <td>0.075025</td>\n",
              "      <td>0.097730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046397</td>\n",
              "      <td>0.042448</td>\n",
              "      <td>0.034551</td>\n",
              "      <td>0.064166</td>\n",
              "      <td>0.047384</td>\n",
              "      <td>0.048371</td>\n",
              "      <td>0.038500</td>\n",
              "      <td>0.035538</td>\n",
              "      <td>0.035538</td>\n",
              "      <td>0.039487</td>\n",
              "      <td>0.042448</td>\n",
              "      <td>0.042448</td>\n",
              "      <td>0.039487</td>\n",
              "      <td>0.041461</td>\n",
              "      <td>0.046397</td>\n",
              "      <td>0.046397</td>\n",
              "      <td>0.049358</td>\n",
              "      <td>0.046397</td>\n",
              "      <td>0.044423</td>\n",
              "      <td>0.045410</td>\n",
              "      <td>0.042448</td>\n",
              "      <td>0.044423</td>\n",
              "      <td>0.038500</td>\n",
              "      <td>0.033564</td>\n",
              "      <td>0.028628</td>\n",
              "      <td>0.033564</td>\n",
              "      <td>0.032577</td>\n",
              "      <td>0.027641</td>\n",
              "      <td>0.034551</td>\n",
              "      <td>0.033564</td>\n",
              "      <td>0.031589</td>\n",
              "      <td>0.034551</td>\n",
              "      <td>0.035538</td>\n",
              "      <td>0.031589</td>\n",
              "      <td>0.032577</td>\n",
              "      <td>0.031589</td>\n",
              "      <td>0.034551</td>\n",
              "      <td>0.030602</td>\n",
              "      <td>0.032577</td>\n",
              "      <td>0.033564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZGNX</th>\n",
              "      <td>0.823704</td>\n",
              "      <td>0.827222</td>\n",
              "      <td>0.839815</td>\n",
              "      <td>0.852778</td>\n",
              "      <td>0.875556</td>\n",
              "      <td>0.883889</td>\n",
              "      <td>0.874259</td>\n",
              "      <td>0.911852</td>\n",
              "      <td>0.882037</td>\n",
              "      <td>0.874074</td>\n",
              "      <td>0.879630</td>\n",
              "      <td>0.869815</td>\n",
              "      <td>0.829259</td>\n",
              "      <td>0.779259</td>\n",
              "      <td>0.799259</td>\n",
              "      <td>0.802593</td>\n",
              "      <td>0.792222</td>\n",
              "      <td>0.796667</td>\n",
              "      <td>0.797593</td>\n",
              "      <td>0.802778</td>\n",
              "      <td>0.845370</td>\n",
              "      <td>0.838333</td>\n",
              "      <td>0.458704</td>\n",
              "      <td>0.417778</td>\n",
              "      <td>0.422963</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.461111</td>\n",
              "      <td>0.459815</td>\n",
              "      <td>0.419444</td>\n",
              "      <td>0.427593</td>\n",
              "      <td>0.436481</td>\n",
              "      <td>0.422407</td>\n",
              "      <td>0.398889</td>\n",
              "      <td>0.382407</td>\n",
              "      <td>0.346481</td>\n",
              "      <td>0.302593</td>\n",
              "      <td>0.328333</td>\n",
              "      <td>0.331481</td>\n",
              "      <td>0.325370</td>\n",
              "      <td>0.370556</td>\n",
              "      <td>...</td>\n",
              "      <td>0.272778</td>\n",
              "      <td>0.264074</td>\n",
              "      <td>0.261111</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.266296</td>\n",
              "      <td>0.250185</td>\n",
              "      <td>0.247222</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>0.254630</td>\n",
              "      <td>0.248889</td>\n",
              "      <td>0.235185</td>\n",
              "      <td>0.231667</td>\n",
              "      <td>0.232037</td>\n",
              "      <td>0.249074</td>\n",
              "      <td>0.256482</td>\n",
              "      <td>0.254259</td>\n",
              "      <td>0.263148</td>\n",
              "      <td>0.260741</td>\n",
              "      <td>0.262778</td>\n",
              "      <td>0.261852</td>\n",
              "      <td>0.272778</td>\n",
              "      <td>0.275556</td>\n",
              "      <td>0.260556</td>\n",
              "      <td>0.255185</td>\n",
              "      <td>0.251852</td>\n",
              "      <td>0.248519</td>\n",
              "      <td>0.233148</td>\n",
              "      <td>0.255556</td>\n",
              "      <td>0.253889</td>\n",
              "      <td>0.234259</td>\n",
              "      <td>0.239815</td>\n",
              "      <td>0.234074</td>\n",
              "      <td>0.238148</td>\n",
              "      <td>0.239630</td>\n",
              "      <td>0.231852</td>\n",
              "      <td>0.237593</td>\n",
              "      <td>0.233519</td>\n",
              "      <td>0.225556</td>\n",
              "      <td>0.231852</td>\n",
              "      <td>0.234074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  250 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       247       248       249\n",
              "INO   0.039704  0.039367  0.038022  ...  0.233849  0.236541  0.231494\n",
              "ALKS  0.090340  0.092874  0.091682  ...  0.114788  0.110167  0.102713\n",
              "AERI  0.246064  0.238345  0.252393  ...  0.065298  0.069003  0.067768\n",
              "TXMD  0.137216  0.142152  0.141165  ...  0.030602  0.032577  0.033564\n",
              "ZGNX  0.823704  0.827222  0.839815  ...  0.225556  0.231852  0.234074\n",
              "\n",
              "[5 rows x 250 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5rlnyIN08a",
        "outputId": "5695d569-548a-4e8d-8fa8-e45aae1cebe7"
      },
      "source": [
        "X_valid.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTWaqBLGDx56"
      },
      "source": [
        "from scipy.linalg import norm\n",
        "\n",
        "result=[]\n",
        "for stock in X_valid:\n",
        "  #reshape data to fit the model\n",
        "  stock = stock.reshape(1,250)\n",
        "\n",
        "  # evaluate the model\n",
        "  stock_output = autoencoder.predict(stock)\n",
        "  #use mse will give the same ranking\n",
        "  #test_mse = mean_squared_error(stock, stock_output)\n",
        "  l2_norm = norm(stock_output-stock,2)\n",
        "  result.append(l2_norm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "v3SoWif6OHe9",
        "outputId": "fd65ff6b-7865-408a-c859-5fa0c3403482"
      },
      "source": [
        "result_df = pd.DataFrame(result,index=tickers)\n",
        "result_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>INO</th>\n",
              "      <td>3.368670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALKS</th>\n",
              "      <td>4.137610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AERI</th>\n",
              "      <td>3.463472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TXMD</th>\n",
              "      <td>4.156068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZGNX</th>\n",
              "      <td>2.131560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OPK</th>\n",
              "      <td>4.169006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CCXI</th>\n",
              "      <td>3.817046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ESPR</th>\n",
              "      <td>1.885321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AMGN</th>\n",
              "      <td>4.227957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BMRN</th>\n",
              "      <td>4.522545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>118 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "INO   3.368670\n",
              "ALKS  4.137610\n",
              "AERI  3.463472\n",
              "TXMD  4.156068\n",
              "ZGNX  2.131560\n",
              "...        ...\n",
              "OPK   4.169006\n",
              "CCXI  3.817046\n",
              "ESPR  1.885321\n",
              "AMGN  4.227957\n",
              "BMRN  4.522545\n",
              "\n",
              "[118 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAALUWGHRE5-"
      },
      "source": [
        "most_communal=result_df.nsmallest(5,0).index.tolist()\n",
        "least_communal=result_df.nlargest(20,0).index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtuomwRDDyJL"
      },
      "source": [
        "## 4. Prints the ticker symbols for selected stocks as follows:\n",
        "Most communal \n",
        "------------- \n",
        "STK1, STK2, STK3, STK4, ... \n",
        "Least communal \n",
        "------------- \n",
        "STK6, STK7, STK8, STK9, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tr1BlrcDy7Q",
        "outputId": "6fe1474e-297a-48a7-a580-17cd25768c5c"
      },
      "source": [
        "print('Most communal')\n",
        "print('-------------')\n",
        "print(most_communal)\n",
        "print(\"\\n\")\n",
        "print('Least communal')\n",
        "print('-------------')\n",
        "print(least_communal)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most communal\n",
            "-------------\n",
            "['CARA', 'AXON', 'ITCI', 'VCYT', 'CHRS']\n",
            "\n",
            "\n",
            "Least communal\n",
            "-------------\n",
            "['PTCT', 'ASND', 'ACAD', 'PBYI', 'CLDX', 'BLCM', 'TLGT', 'ACOR', 'CMRX', 'AUPH', 'CLVS', 'ENDP', 'BMRN', 'MNKD', 'AKBA', 'AMGN', 'BPMC', 'OPK', 'TXMD', 'LGND']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72IYviRDSyWh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}